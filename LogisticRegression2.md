# ğŸ” Logistic Regression Deep Dive â€” From Sigmoid to Accuracy  
> **â€œWhen probabilities meet decisions, math gets strategic.â€** ğŸ˜…ğŸ“Š

This page unpacks the mechanics behind logistic regression â€” from sigmoid curves to classification thresholds, logit functions, and evaluation metrics.

---

<details>
<summary>ğŸ“ Sigmoid Function â€” The Probability Gate</summary>

### ğŸ¯ What It Does  
Transforms any real-valued number into a probability between 0 and 1.  
The **variance** of the sigmoid output reflects how uncertain the prediction is â€” highest near 0.5, lowest near 0 or 1.

---

### ğŸ§® Sigmoid Variance Equation

$$
\sigma^2(a) = \frac{1}{(1 + e^{-a})^2}, \quad \text{where } a = -\beta_0 - \beta_1 x_1 + \dots + \beta_n x_n
$$

---

### ğŸ“ Annotated Terms in Sigmoid Variance Equation

### ğŸ“ Annotated Terms in Sigmoid Variance Equation

| Symbol                          | Meaning                                                                 |
|----------------------------------|-------------------------------------------------------------------------|
| \(\sigma^2(a)\)                 | Variance of the sigmoid output â€” reflects uncertainty in prediction     |
| \(e^{-a}\)                      | Exponential decay â€” controls the steepness of the sigmoid curve         |
| \(a\)                           | Linear combination of inputs and weights â€” raw model score              |
| \(\beta_0\)                     | Intercept term â€” baseline bias                                          |
| \(\beta_1, \dots, \beta_n\)     | Coefficients â€” influence of each feature on the prediction              |
| \(x_1, \dots, x_n\)             | Input features â€” the data used to make predictions                      |


---

### â˜• Intuition  
As \(a\) increases, the sigmoid output approaches 1 and variance drops â€” Kanakâ€™s confidence rises.  
As \(a\) nears 0, the output hovers around 0.5 â€” uncertainty peaks, like debugging pre-chai.

</details>


---

<details>
<summary>ğŸ”¢ Probabilities, Classes & Thresholds</summary>

### ğŸ§  How It Works  
Logistic regression outputs a probability \( \hat{p} \) for class membership.  
We convert this into a class label using a **threshold** (default = 0.5):

- If \( \hat{p} > \text{threshold} \) â†’ Class 1 (e.g., â€œStressedâ€)  
- If \( \hat{p} < \text{threshold} \) â†’ Class 0 (e.g., â€œCalmâ€)

### âš–ï¸ Implications of Changing the Threshold  
| Threshold â†‘ | Threshold â†“ |
|-------------|-------------|
| Fewer positives predicted | More positives predicted |
| Higher precision | Higher recall |
| May miss true positives | May include false positives |

</details>

---

<details>
<summary>ğŸ“ˆ Decision Boundary</summary>

### ğŸ¯ What It Is  
The point where the predicted probability crosses the threshold â€” typically where \( \hat{p} = 0.5 \).  
In 2D, itâ€™s a line; in higher dimensions, itâ€™s a hyperplane.

### â˜• Analogy  
Itâ€™s like Kanakâ€™s debugging threshold â€” below 0.5, itâ€™s â€œprobably fineâ€; above 0.5, itâ€™s â€œtime to refactor.â€

</details>

---

<details>
<summary>ğŸ“Š Logit Function â€” Why It Matters</summary>

### ğŸ§  Definition  
The **logit** is the inverse of the sigmoid:



\[
\text{logit}(p) = \log\left(\frac{p}{1 - p}\right)
\]



### ğŸ¯ Why Use It  
- Converts probabilities to **log-odds**  
- Makes the model **linear in parameters**  
- Enables interpretation of coefficients

</details>

---

<details>
<summary>ğŸ² Odds â€” The Intuition</summary>

### ğŸ§  What Are Odds?  
Odds represent the ratio of success to failure:



\[
\text{Odds} = \frac{p}{1 - p}
\]



### ğŸ“Š Example  
- \( p = 0.8 \) â†’ Odds = 4 (4 times more likely to be class 1)  
- \( p = 0.2 \) â†’ Odds = 0.25 (less likely to be class 1)

</details>

---

<details>
<summary>ğŸš¦ Classifications & Misclassifications</summary>

### âœ… True Positives (TP)  
Correctly predicted class 1

### âŒ False Positives (FP)  
Predicted class 1, but itâ€™s actually class 0

### âŒ False Negatives (FN)  
Predicted class 0, but itâ€™s actually class 1

### âœ… True Negatives (TN)  
Correctly predicted class 0

</details>

---

<details>
<summary>ğŸ“ Classification Accuracy</summary>

### ğŸ§® Formula  


\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]



### âš ï¸ Caveat  
Accuracy can be misleading with **imbalanced classes** â€” use **precision**, **recall**, and **F1 score** for deeper insight.

</details>

---

<details>
<summary>ğŸ¯ Class of Interest</summary>

### ğŸ§  What It Means  
The class you care most about â€” often the **positive class** (e.g., â€œStressedâ€, â€œChurnâ€, â€œDiseaseâ€).

### ğŸ“Š Why It Matters  
- Guides threshold tuning  
- Impacts metric selection  
- Shapes model interpretation

</details>

---

### ğŸ¬ Whatâ€™s Next?

- ğŸ“ˆ Visualize sigmoid curves and decision boundaries  
- âœ‚ï¸ Tune thresholds for precision vs recall trade-offs  
- ğŸ§  Extend to **multiclass and multilabel** setups  
- â˜• Build a chai-themed classifier: â€œWill Kanak need caffeine?â€

---

<details>
<summary>ğŸ“ˆ Sigmoid Curve â€” Probability vs Raw Score</summary>

### ğŸ¯ What It Shows  
The sigmoid function maps any input \( a \) to a probability between 0 and 1:



\[
\sigma(a) = \frac{1}{1 + e^{-a}}
\]



### ğŸ“Š Plot Characteristics  
- S-shaped curve  
- Centered at \( a = 0 \) â†’ probability = 0.5  
- As \( a \to +\infty \), \( \sigma(a) \to 1 \)  
- As \( a \to -\infty \), \( \sigma(a) \to 0 \)

### â˜• Intuition  
Think of it as Kanakâ€™s confidence meter â€” low \( a \) = uncertain, high \( a \) = confident prediction.

### ğŸ§‘â€ğŸ’» Python Snippet

```python
import numpy as np
import matplotlib.pyplot as plt

a = np.linspace(-10, 10, 100)
sigmoid = 1 / (1 + np.exp(-a))

plt.plot(a, sigmoid, label='Sigmoid Curve')
plt.axhline(0.5, color='gray', linestyle='--', label='Threshold = 0.5')
plt.xlabel('Raw Score (a)')
plt.ylabel('Probability')
plt.title('Sigmoid Function')
plt.legend()
plt.grid(True)
plt.show()
```
</details>


---

```markdown
<details>
<summary>ğŸ§­ Decision Boundary â€” Classification Threshold</summary>

### ğŸ¯ What It Means  
The **decision boundary** is the point where the predicted probability crosses the classification threshold (usually 0.5).

### ğŸ“ In 1D  
- If \( \sigma(a) > 0.5 \) â†’ Class 1  
- If \( \sigma(a) < 0.5 \) â†’ Class 0

### ğŸ“ In 2D  
- The boundary becomes a **line** separating two regions  
- In higher dimensions, itâ€™s a **hyperplane**

### ğŸ§‘â€ğŸ’» Python Snippet (2D Visualization)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1)
model = LogisticRegression()
model.fit(X, y)

# Plot decision boundary
coef = model.coef_[0]
intercept = model.intercept_
x_vals = np.linspace(X[:,0].min(), X[:,0].max(), 100)
y_vals = -(coef[0] * x_vals + intercept) / coef[1]

plt.scatter(X[:,0], X[:,1], c=y, cmap='bwr', alpha=0.7)
plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Logistic Regression Decision Boundary')
plt.legend()
plt.grid(True)
plt.show()

/<details>
